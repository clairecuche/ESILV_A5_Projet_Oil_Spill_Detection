import sys
from pathlib import Path

import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
import matplotlib.pyplot as plt
from ultralytics import YOLO
import cv2

# Ajoute la racine du projet au chemin de recherche de Python
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data.data_loaders import get_dataloaders
from src.data.preprocessing import calculate_class_weights, convert_coco_to_yolo_segmentation
from config import *
from metrics import SegmentationMetrics


class YOLOv11Trainer:
    """
    EntraÃ®neur YOLOv11 pour la segmentation d'instance selon les paramÃ¨tres du paper LADOS.
    Utilise weighted BCE loss et les mÃªmes mÃ©triques (IoU, mAcc) que SegFormer.
    """
    
    def __init__(self):
        self.device = torch.device(DEVICE)
        
        # âœ… FIX 1: Clarifier les num_classes
        # YOLO entraÃ®ne sur 5 classes (sans Background)
        # Mais on Ã©value sur 6 classes (avec Background) pour comparer avec SegFormer
        self.num_classes_yolo = NUM_CLASSES_YOLO  # 5 (pour YOLO)
        self.num_classes_eval = NUM_CLASSES       # 6 (pour Ã©valuation)
        
        # 1. ModÃ¨le YOLOv11 (prÃ©-entraÃ®nÃ© sur COCO)
        self.model = self._initialize_model()
        
        # 2. Poids de classe pour weighted BCE loss
        weights_path = DATA_DIR / 'yolo_class_weights.pt'
        if weights_path.exists():
            self.class_weights = torch.load(weights_path)
        else:
            # Calculer les poids pour les 5 classes d'objets (sans BG)
            self.class_weights = calculate_class_weights(split='train', use_paper_method=True)
            # Si la fonction renvoie 6 poids (avec BG), on prend les 5 derniers
            if len(self.class_weights) == 6:
                self.class_weights = self.class_weights[1:]
            torch.save(self.class_weights, weights_path)
        
        # 3. MÃ©triques et Suivi
        # âœ… FIX 2: Utiliser 6 classes pour l'Ã©valuation (cohÃ©rent avec GT)
        self.val_metrics = SegmentationMetrics(num_classes=self.num_classes_eval)
        self.best_miou = -1.0
        self.best_map = -1.0
        self.patience_counter = 0
        self.history = {
            'train_loss': [], 
            'val_loss': [], 
            'train_miou': [], 
            'val_miou': [],
            'val_map50': [],
            'val_map50_95': []
        }
        
        # 4. Directories
        self.checkpoint_dir = OUTPUT_DIR_YOLO / 'checkpoints'
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"YOLOv11 Trainer initialisÃ© sur {self.device}.")
        print(f"Classes YOLO (entraÃ®nement): {self.num_classes_yolo}")
        print(f"Classes Ã©valuation (avec BG): {self.num_classes_eval}")
        print(f"Poids de classe: {self.class_weights}")

    def _initialize_model(self):
        """Initialise YOLOv11m-seg avec poids COCO."""
        model = YOLO('yolo11m-seg.pt')  # YOLOv11m segmentation
        return model
    
    def _prepare_yolo_format(self):
        """
        PrÃ©pare les donnÃ©es au format YOLO (conversion des masques en polygones).
        Utilise le module yolo_data_converter.
        """
        yolo_data_dir = DATA_DIR / 'yolo_format'
        yaml_path = yolo_data_dir / 'data.yaml'
        
        # VÃ©rifier si la conversion est dÃ©jÃ  faite
        if yaml_path.exists():
            print(f"âœ… DonnÃ©es YOLO dÃ©jÃ  converties: {yolo_data_dir}")
            return str(yaml_path)
        
        # Sinon, lancer la conversion
        print("ðŸ”„ Conversion COCO â†’ YOLO en cours...")
        convert_coco_to_yolo_segmentation(DATA_DIR, yolo_data_dir)
        
        return str(yaml_path)
    
    def train(self):
        """
        EntraÃ®nement YOLOv11 avec les paramÃ¨tres du paper LADOS:
        - Batch size: 32
        - Epochs: max 80 (avec early stopping patience=10)
        - Optimizer: AdamW implicite dans YOLO
        - Augmentations: mosaic, HSV, flip, translate, scale
        - Early stopping basÃ© sur weighted average mAP (0.1*mAP@50 + 0.9*mAP@50-95)
        """
        print("\nðŸš€ DÃ©but de l'entraÃ®nement YOLOv11...")
        start_time = datetime.now()
        
        # PrÃ©parer le format YOLO
        data_yaml_path = self._prepare_yolo_format()
        
        # Configuration d'entraÃ®nement selon le paper
        results = self.model.train(
            data=data_yaml_path,
            epochs=NUM_EPOCHS_YOLO,  # Max epochs (early stopping activÃ©)
            batch=BATCH_SIZE_YOLO,   # 32 selon paper
            imgsz=TARGET_SIZE[0],
            device=self.device,
            
            # Optimizer (AdamW implicite)
            optimizer='AdamW',
            lr0=LEARNING_RATE_YOLO,
            weight_decay=WEIGHT_DECAY,
            
            # Early stopping (patience=10 selon paper)
            patience=PATIENCE,
            
            # Augmentations (selon paper section 4.3)
            mosaic=1.0,   # Mosaic augmentation
            hsv_h=0.015,  # HSV-Hue augmentation
            hsv_s=0.7,    # HSV-Saturation
            hsv_v=0.4,    # HSV-Value
            degrees=0.0,  # Rotation (pas mentionnÃ© dans paper)
            translate=0.1,  # Translation
            scale=0.5,    # Scaling
            fliplr=0.5,   # Horizontal flip
            
            # Loss weights (BCE pour masks)
            box=7.5,
            cls=0.5,
            dfl=1.5,
            
            # Sauvegarde
            project=str(OUTPUT_DIR_YOLO),
            name='yolo_training',
            exist_ok=True,
            save=True,
            save_period=5,
            
            # Validation
            val=True,
            plots=True,
            verbose=True
        )
        
        training_time = (datetime.now() - start_time).total_seconds() / 3600
        print(f"\n--- ENTRAÃŽNEMENT TERMINÃ‰ ---")
        print(f"Temps total : {training_time:.2f} heures.")
        
        # Ã‰valuation finale sur test set
        self._evaluate_with_metrics()
        
        return results

    def _evaluate_with_metrics(self):
        """
        Ã‰value le modÃ¨le avec dÃ©normalisation et conversion sÃ©mantique CORRIGÃ‰E.
        """
        print("\n" + "="*50)
        print("ðŸ“Š Ã‰VALUATION FINALE SUR TEST SET")
        print("="*50)
        
        # 1. Charger le meilleur modÃ¨le sauvegardÃ©
        # âœ… Essayer plusieurs chemins possibles
        possible_paths = [
            OUTPUT_DIR_YOLO / 'yolo_training' / 'weights' / 'best.pt',
            Path("/kaggle/working/ESILV_A5_Projet_Oil_Spill_Detection/outputs/yolo/yolo_training/weights/best.pt"),
            Path(r"C:\Users\benoi\OneDrive - De Vinci\A5 ESILV\CV\Project\ESILV_A5_Projet_Oil_Spill_Detection\outputs\yolo\yolo_training\weights\best.pt"),
            Path("/kaggle/working/yolo/yolo_training/weights/best.pt"),
            Path("output/best.pt")
        ]
        
        best_model_path = None
        for path in possible_paths:
            if path.exists():
                best_model_path = path
                break
        
        if best_model_path is None:
            raise FileNotFoundError("âŒ Aucun modÃ¨le YOLO trouvÃ© dans les chemins possibles!")
            
        print(f"ðŸ”„ Chargement des poids : {best_model_path}")
        model = YOLO(str(best_model_path))

        # 2. ParamÃ¨tres de dÃ©normalisation (ImageNet standards)
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        
        # 3. Charger le test loader
        test_loader = get_dataloaders()['test']
        
        # âœ… FIX 3: VÃ©rifier qu'on est bien sur le test set
        print(f"\nðŸ” VÃ©rification du split:")
        print(f"   Nombre d'images test: {len(test_loader.dataset)}")
        print(f"   Attendu (paper LADOS): 343 images")
        if len(test_loader.dataset) != 343:
            print(f"   âš ï¸ WARNING: Le nombre d'images ne correspond pas au paper!")
        
        self.val_metrics.reset()
        
        # âœ… FIX 4: Compteurs pour debug
        total_predictions = 0
        total_background_pixels = 0
        total_non_background_pixels = 0
        
        print("\nðŸ” PrÃ©diction et conversion sÃ©mantique...")
        with torch.no_grad():
            for batch_idx, (images, masks) in enumerate(tqdm(test_loader, desc="Testing")):
                for i in range(images.shape[0]):
                    # === DÃ‰NORMALISATION CORRECTE ===
                    img_np = images[i].permute(1, 2, 0).cpu().numpy()
                    img_denorm = img_np * std + mean
                    img_uint8 = (img_denorm * 255).clip(0, 255).astype(np.uint8)
                    
                    # === PRÃ‰DICTION YOLO AVEC SEUILS OPTIMISÃ‰S ===
                    results = model.predict(
                        img_uint8,
                        verbose=False,
                        device=self.device,
                        imgsz=TARGET_SIZE[0],
                        conf=0.15,  # âœ… Seuil bas pour plus de dÃ©tections
                        iou=0.4     # âœ… NMS moins agressif
                    )
                    
                    # === CONVERSION INSTANCE â†’ SÃ‰MANTIQUE AVEC MAPPING ===
                    gt_mask = masks[i].cpu().numpy()
                    pred_semantic = self._convert_instance_to_semantic(
                        results[0],
                        gt_mask.shape
                    )
                    
                    # Debug: Compter les dÃ©tections
                    if results[0].boxes is not None:
                        total_predictions += len(results[0].boxes)
                    
                    bg_pixels = (pred_semantic == 0).sum()
                    non_bg_pixels = (pred_semantic > 0).sum()
                    total_background_pixels += bg_pixels
                    total_non_background_pixels += non_bg_pixels
                    
                    # Debug premiÃ¨re image
                    if batch_idx == 0 and i == 0:
                        print(f"\n[DEBUG] PremiÃ¨re prÃ©diction:")
                        print(f"  Image shape: {img_uint8.shape}")
                        print(f"  GT mask shape: {gt_mask.shape}")
                        print(f"  GT classes: {np.unique(gt_mask)}")
                        print(f"  Pred classes: {np.unique(pred_semantic)}")
                        if results[0].boxes is not None:
                            print(f"  Nb dÃ©tections YOLO: {len(results[0].boxes)}")
                            print(f"  Classes YOLO (0-4): {results[0].boxes.cls.cpu().numpy()[:5]}")
                            print(f"  Confidences: {results[0].boxes.conf.cpu().numpy()[:5]}")
                        print(f"  Pixels Background: {bg_pixels} ({bg_pixels/(gt_mask.size)*100:.1f}%)")
                        print(f"  Pixels dÃ©tectÃ©s: {non_bg_pixels} ({non_bg_pixels/(gt_mask.size)*100:.1f}%)")
                    
                    # Mise Ã  jour mÃ©triques
                    self.val_metrics.update(pred_semantic, gt_mask)
        
        # 4. Statistiques globales
        total_pixels = total_background_pixels + total_non_background_pixels
        print(f"\nðŸ“Š Statistiques globales:")
        print(f"   Total dÃ©tections YOLO: {total_predictions}")
        print(f"   Pixels Background: {total_background_pixels} ({total_background_pixels/total_pixels*100:.1f}%)")
        print(f"   Pixels dÃ©tectÃ©s: {total_non_background_pixels} ({total_non_background_pixels/total_pixels*100:.1f}%)")
        
        # 5. RÃ©sultats finaux
        final_results = self.val_metrics.get_results()
        
        print(f"\nâœ… mIoU Final (excl. Background) : {final_results['mIoU']:.4f}")
        print(f"âœ… mAcc Final : {final_results['mAcc']:.4f}")
        
        print("\nðŸ“‹ IoU par classe:")
        for i, iou in enumerate(final_results['class_iou']):
            print(f"  {CLASS_NAMES_SEG[i]:15s}: {iou:.4f}")
        
        # 6. Matrice de confusion (optionnel mais utile)
        print("\nðŸ“Š Matrice de Confusion (premiÃ¨res lignes):")
        conf_matrix = self.val_metrics.confusion_matrix
        for i in range(min(6, self.num_classes_eval)):
            true_class = CLASS_NAMES_SEG[i]
            total = conf_matrix[i].sum()
            if total > 0:
                print(f"\n{true_class} (GT) - Total pixels: {total:.0f}")
                for j in range(min(6, self.num_classes_eval)):
                    pred_class = CLASS_NAMES_SEG[j]
                    count = conf_matrix[i, j]
                    percentage = (count / total * 100) if total > 0 else 0
                    if percentage > 5:  # Afficher si > 5%
                        print(f"  â†’ {pred_class:15s}: {percentage:5.1f}%")
        
        # 7. Sauvegarde JSON
        results_path = OUTPUT_DIR_YOLO / 'test_results.json'
        with open(results_path, 'w') as f:
            json.dump({
                'mIoU': float(final_results['mIoU']),
                'mAcc': float(final_results['mAcc']),
                'class_iou': [float(x) for x in final_results['class_iou']],
                'class_names': [CLASS_NAMES_SEG[i] for i in range(self.num_classes_eval)],
                'conf_threshold': 0.15,
                'iou_threshold': 0.4,
                'total_detections': int(total_predictions),
                'background_ratio': float(total_background_pixels / total_pixels)
            }, f, indent=2)
            
        print(f"\nðŸ’¾ RÃ©sultats validÃ©s sauvegardÃ©s dans : {results_path}")
    
    def _convert_instance_to_semantic(self, result, img_shape):
        """
        Convertit les masques d'instance YOLO en masque sÃ©mantique.
        
        âœ… FIX CRITIQUE: Mapper YOLO classes (0-4) â†’ GT classes (1-5)
        
        Selon le paper (section 4.4):
        "we assigned each predicted pixel the class with the highest confidence score"
        
        Args:
            result: RÃ©sultat YOLO (contient masks, boxes, cls, conf)
            img_shape: (H, W) de l'image
            
        Returns:
            semantic_mask: np.ndarray de shape (H, W) avec les class IDs GT (0-5)
        """
        H, W = img_shape
        semantic_mask = np.zeros((H, W), dtype=np.int64)  # Background = 0 par dÃ©faut
        confidence_map = np.zeros((H, W), dtype=np.float32)
        
        if result.masks is None:
            return semantic_mask
        
        # RÃ©cupÃ©rer les masques, classes et confidences
        masks = result.masks.data.cpu().numpy()  # (N, H, W)
        classes = result.boxes.cls.cpu().numpy().astype(int)  # (N,) - Classes YOLO 0-4
        confidences = result.boxes.conf.cpu().numpy()  # (N,)
        
        # Redimensionner les masques Ã  la taille de l'image
        for mask, yolo_cls, conf in zip(masks, classes, confidences):
            # Resize mask
            mask_resized = cv2.resize(mask, (W, H), interpolation=cv2.INTER_LINEAR)
            mask_binary = mask_resized > 0.5
            
            # âœ… FIX CRITIQUE: MAPPING YOLO â†’ GT
            # YOLO classes: 0=Oil, 1=Emulsion, 2=Sheen, 3=Ship, 4=Oil-platform
            # GT classes:   1=Oil, 2=Emulsion, 3=Sheen, 4=Ship, 5=Oil-platform
            # Mapping simple: gt_cls = yolo_cls + 1
            gt_cls = yolo_cls +1
            
            # Pour chaque pixel, garder la classe avec la plus haute confiance
            update_mask = (mask_binary) & (conf > confidence_map)
            semantic_mask[update_mask] = gt_cls
            confidence_map[update_mask] = conf
        
        return semantic_mask


if __name__ == '__main__':
    trainer = YOLOv11Trainer()
    trainer.train()
    
    